import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K

# å‚æ•°è®¾ç½®
MAX_NUM_WORDS = 20000
MAX_SEQUENCE_LENGTH = 100
EMBEDDING_DIM = 100
EPOCHS = 5
BATCH_SIZE = 32

# === Attention Layer === #
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight",
                                 shape=(input_shape[-1], 1),
                                 initializer="glorot_uniform",
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, x, return_attention=False):
        e = K.tanh(K.dot(x, self.W))           # (batch, time_steps, 1)
        a = K.softmax(e, axis=1)               # æ³¨æ„åŠ›æƒé‡ (batch, time_steps, 1)
        context = x * a                        # åŠ æƒåçš„è¡¨ç¤º
        context = K.sum(context, axis=1)       # æ±‡æ€»ä¸ºä¸€ä¸ªå‘é‡

        if return_attention:
            return context, a
        return context

# === è¯»å–å’Œå‡†å¤‡æ•°æ® === #
df = pd.read_csv("train.csv")
df.fillna("", inplace=True)
df["full_text"] = df["keyword"] + " " + df["location"] + " " + df["text"]
texts = df["full_text"].values
labels = df["target"].values

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
y = labels

# è®­ç»ƒéªŒè¯é›†åˆ’åˆ†
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# === æ„å»ºæ¨¡å‹ï¼ˆè®­ç»ƒæ¨¡å‹ï¼‰ === #
input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))
embedding = Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM)(input_)
lstm_out = LSTM(64, return_sequences=True)(embedding)

# ç”¨ Attention å±‚è¾“å‡º context vector
att_layer = AttentionLayer()
context = att_layer(lstm_out)

x = Dropout(0.5)(context)
x = Dense(32, activation="relu")(x)
output = Dense(1, activation="sigmoid")(x)

model = Model(inputs=input_, outputs=output)
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
model.summary()

# === è®­ç»ƒ === #
model.fit(X_train, y_train,
          validation_data=(X_val, y_val),
          epochs=EPOCHS,
          batch_size=BATCH_SIZE)

model.save("tweet_attention_model_v2.h5")

# === æ„å»º Attention æƒé‡è¾“å‡ºæ¨¡å‹ === #
# Attention å±‚é‡æ–°è°ƒç”¨ï¼Œå¸¦ return_attention=True
att_layer_extract = AttentionLayer()
context2, att_weights = att_layer_extract(lstm_out, return_attention=True)
att_model = Model(inputs=input_, outputs=[att_weights, output])

# === ä»éªŒè¯é›†ä¸­æå–å…³é”®è¯ï¼ˆä¿®å¤ padding é—®é¢˜ï¼‰=== #
word_index = tokenizer.word_index
reverse_word_index = {v: k for k, v in word_index.items()}
keyword_counter = {}
total_extracted = 0

for i in range(min(200, len(X_val))):
    sample_input = X_val[i:i+1]
    att_w, pred = att_model.predict(sample_input, verbose=0)
    weights = att_w[0].squeeze()  # é•¿åº¦åº”ä¸º MAX_SEQUENCE_LENGTH

    token_ids = sample_input[0]
    tokens = [reverse_word_index.get(tid, "<PAD>") for tid in token_ids]

    # æ‰“å°ä¸€æ¡æ ·æœ¬ attention çŠ¶æ€ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    if i == 0:
        print("\nğŸ“Œ ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ Token å’Œ Attention:")
        for tok, weight in zip(tokens, weights):
            print(f"{tok:>15}: {weight:.4f}")

    # è·å– top3 é«˜æƒé‡ç´¢å¼•ï¼ˆä¸è·³ PADï¼‰
    top_indices = weights.argsort()[-3:][::-1]

    for idx in top_indices:
        word = tokens[idx]
        if word != "<PAD>":
            keyword_counter[word] = keyword_counter.get(word, 0) + 1
            total_extracted += 1

print(f"\nâœ… å…±æå–å…³é”®è¯æ•°ï¼ˆéå»é‡ï¼‰ï¼š{total_extracted}")

# ç»Ÿè®¡å‰50å…³é”®è¯
sorted_keywords = sorted(keyword_counter.items(), key=lambda x: -x[1])[:50]
if sorted_keywords:
    with open("attention_keywords.json", "w") as f:
        json.dump(sorted_keywords, f, indent=2)
    print("\nâœ… attention_keywords.json å·²ä¿å­˜")
else:
    print("\nâš ï¸ æ²¡æœ‰æœ‰æ•ˆå…³é”®è¯è¢«æå–ï¼Œå¯èƒ½æ˜¯æ¨¡å‹æ²¡å­¦ä¼šæˆ–è€…è¾“å…¥å¤ªçŸ­")

