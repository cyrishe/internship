import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import os

class TweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt"
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }

print("è¯»å–æ•°æ®...")
df = pd.read_csv("train.csv")
df.fillna("", inplace=True)
df['combined'] = df['text'] + " " + df['keyword'] + " " + df['location']
texts = df['combined'].tolist()
labels = df['target'].tolist()

print("åŠ è½½æ¨¡å‹...")
tokenizer = BertTokenizer.from_pretrained("./bert_manual_model")
model = BertForSequenceClassification.from_pretrained("./bert_manual_model")

full_dataset = TweetDataset(texts, labels, tokenizer)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

print("é…ç½®è®¾å¤‡...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("å½“å‰ä½¿ç”¨è®¾å¤‡:", device)
model = model.to(device)
optimizer = AdamW(model.parameters(), lr=2e-5)
epochs = 2

print("å¼€å§‹è®­ç»ƒ...")
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Training]"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_loader)

    model.eval()
    val_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Validation]"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            val_loss += outputs.loss.item()
            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    avg_val_loss = val_loss / len(val_loader)
    val_acc = correct / total
    print(f"\nâœ… Epoch {epoch+1} Summary:")
    print(f"  ğŸŸ¢ Train Loss: {avg_train_loss:.4f}")
    print(f"  ğŸ”µ Val   Loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.4f}\n")

output_dir = "./bert_manual_model"
os.makedirs(output_dir, exist_ok=True)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°:", output_dir)
